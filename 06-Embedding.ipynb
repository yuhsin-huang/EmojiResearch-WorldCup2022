{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03977c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import emoji\n",
    "import warnings\n",
    "import nltk\n",
    "from nltk.corpus import stopwords    \n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54a696f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eafc0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data/cleaned_by_language.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdb35f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_df = df[df['language'] == 'en']\n",
    "es_df = df[df['language'] == 'es']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16da499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_comment = en_df['text only'].to_list()\n",
    "es_comment = es_df['text only'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96a0ab7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/aorawancraprayoon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aorawancraprayoon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "en_stop_words = set(stopwords.words('english'))\n",
    "en_filtered_comments = []\n",
    "\n",
    "for comment in en_comment:\n",
    "    # remove word starts with @\n",
    "    comment = \" \".join(filter(lambda x:x[0]!='@', comment.split()))\n",
    "    \n",
    "    # remove punctuations\n",
    "    comment = comment.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # remove stopwords\n",
    "    words = nltk.word_tokenize(comment)\n",
    "    filtered_comment = [word for word in words if word.lower() not in en_stop_words]\n",
    "    filtered_comment = ' '.join(filtered_comment)\n",
    "    filtered_comment = comment\n",
    "    en_filtered_comments.append(filtered_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0edf613",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_stop_words = set(stopwords.words('spanish'))\n",
    "es_filtered_comments = []\n",
    "\n",
    "for comment in es_comment:\n",
    "    # remove word starts with @\n",
    "    comment = \" \".join(filter(lambda x:x[0]!='@', comment.split()))\n",
    "    \n",
    "    # remove punctuations\n",
    "    comment = comment.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # remove stopwords\n",
    "    words = nltk.word_tokenize(comment)\n",
    "    filtered_comment = [word for word in words if word.lower() not in es_stop_words]\n",
    "    filtered_comment = ' '.join(filtered_comment)\n",
    "    filtered_comment = comment\n",
    "    es_filtered_comments.append(filtered_comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d189bc7e",
   "metadata": {},
   "source": [
    "## Text Embeddings using LASER - this pretrained model supports cross-lingual tasks and embeds in setence-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb8a53ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting laserembeddings\n",
      "  Using cached laserembeddings-1.1.2-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: transliterate==1.10.2 in /Users/aorawancraprayoon/Library/Python/3.9/lib/python/site-packages (from laserembeddings) (1.10.2)\n",
      "Requirement already satisfied: subword-nmt<0.4.0,>=0.3.6 in /Users/aorawancraprayoon/Library/Python/3.9/lib/python/site-packages (from laserembeddings) (0.3.8)\n",
      "Requirement already satisfied: sacremoses==0.0.35 in /Users/aorawancraprayoon/Library/Python/3.9/lib/python/site-packages (from laserembeddings) (0.0.35)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.15.4 in /Users/aorawancraprayoon/Library/Python/3.9/lib/python/site-packages (from laserembeddings) (1.26.1)\n",
      "Requirement already satisfied: torch<2.0.0,>=1.0.1.post2 in /Users/aorawancraprayoon/Library/Python/3.9/lib/python/site-packages (from laserembeddings) (1.13.0)\n",
      "Requirement already satisfied: click in /Users/aorawancraprayoon/Library/Python/3.9/lib/python/site-packages (from sacremoses==0.0.35->laserembeddings) (8.1.7)\n",
      "Requirement already satisfied: tqdm in /Users/aorawancraprayoon/Library/Python/3.9/lib/python/site-packages (from sacremoses==0.0.35->laserembeddings) (4.66.1)\n",
      "Requirement already satisfied: six in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from sacremoses==0.0.35->laserembeddings) (1.15.0)\n",
      "Requirement already satisfied: joblib in /Users/aorawancraprayoon/Library/Python/3.9/lib/python/site-packages (from sacremoses==0.0.35->laserembeddings) (1.3.2)\n",
      "Requirement already satisfied: mock in /Users/aorawancraprayoon/Library/Python/3.9/lib/python/site-packages (from subword-nmt<0.4.0,>=0.3.6->laserembeddings) (5.1.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/aorawancraprayoon/Library/Python/3.9/lib/python/site-packages (from torch<2.0.0,>=1.0.1.post2->laserembeddings) (4.7.1)\n",
      "Installing collected packages: laserembeddings\n",
      "Successfully installed laserembeddings-1.1.2\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install laserembeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2592874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "178868e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin:/Users/aorawancraprayoon/Library/Python/3.9/bin:/Users/aorawancraprayoon/anaconda3/bin:/Users/aorawancraprayoon/anaconda3/condabin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/opt/X11/bin:/Library/TeX/texbin:/Applications/Postgres.app/Contents/Versions/latest/bin:/Users/aorawancraprayoon/anaconda3/bin:/Users/aorawancraprayoon/anaconda3/condabin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/opt/X11/bin:/Library/TeX/texbin:/Applications/Postgres.app/Contents/Versions/latest/bin\n"
     ]
    }
   ],
   "source": [
    "!echo $PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0689250f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/aorawancraprayoon/Desktop/EmojiResearch-WorldCup2022', '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python39.zip', '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9', '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/lib-dynload', '', '/Users/aorawancraprayoon/Library/Python/3.9/lib/python/site-packages', '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages', '/Library/Python/3.9/site-packages']\n"
     ]
    }
   ],
   "source": [
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87ff25d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading models into /Users/aorawancraprayoon/Library/Python/3.9/lib/python/site-packages/laserembeddings/data\n",
      "\n",
      "âœ…   Downloaded https://dl.fbaipublicfiles.com/laser/models/93langs.fcodes    \n",
      "âœ…   Downloaded https://dl.fbaipublicfiles.com/laser/models/93langs.fvocab    \n",
      "âœ…   Downloaded https://dl.fbaipublicfiles.com/laser/models/bilstm.93langs.2018-12-26.pt    \n",
      "\n",
      "âœ¨ You're all set!\n"
     ]
    }
   ],
   "source": [
    "!/usr/bin/python3 -m laserembeddings download-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "814bf8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this in terminal first: python -m laserembeddings download-models\n",
    "from laserembeddings import Laser\n",
    "\n",
    "laser = Laser()\n",
    "en_text_array = laser.embed_sentences(en_filtered_comments, lang='en')\n",
    "es_text_array = laser.embed_sentences(es_filtered_comments, lang='es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c280691a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.3.2-cp39-cp39-macosx_11_0_arm64.whl (24.0 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24.0 MB 28.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy>=1.7.0\n",
      "  Downloading scipy-1.11.3-cp39-cp39-macosx_12_0_arm64.whl (29.7 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29.7 MB 23.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting smart-open>=1.8.1\n",
      "  Downloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57 kB 15.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18.5 in /Users/aorawancraprayoon/Library/Python/3.9/lib/python/site-packages (from gensim) (1.26.1)\n",
      "Installing collected packages: smart-open, scipy, gensim\n",
      "Successfully installed gensim-4.3.2 scipy-1.11.3 smart-open-6.4.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7b356d",
   "metadata": {},
   "source": [
    "## Emoji Embeddings using Emoji2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c71241e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "en_emoji = en_df['emoji list'].to_list()\n",
    "\n",
    "# Load pretrained emoji embeddings\n",
    "emoji_model = KeyedVectors.load_word2vec_format('emoji2vec.bin', binary=True)\n",
    "\n",
    "# Initialize a list to store emoji embeddings\n",
    "en_emoji_embedding = []\n",
    "\n",
    "for emoji_list in en_emoji:\n",
    "    emoji_list_embedding = []  # Initialize a list for embeddings of each emoji list\n",
    "    for emoji in emoji_list:\n",
    "        try:\n",
    "            emoji_list_embedding.append(emoji_model[emoji])\n",
    "        except KeyError:\n",
    "            pass\n",
    "    if len(emoji_list_embedding) != 0:\n",
    "        emoji_list_embedding = np.concatenate(emoji_list_embedding, axis=0)\n",
    "    en_emoji_embedding.append(emoji_list_embedding)\n",
    "\n",
    "\n",
    "en_max_size = max(len(arr) for arr in en_emoji_embedding)\n",
    "en_padded_arrays = [np.pad(arr, (0, en_max_size - len(arr)), 'constant') for arr in en_emoji_embedding]\n",
    "en_emoji_array = np.vstack(en_padded_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "17f8995f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(280, 3600)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_emoji_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "30440c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(280, 1024)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_text_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4421f4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_emoji = es_df['emoji list'].to_list()\n",
    "\n",
    "es_emoji_embedding = []\n",
    "\n",
    "for emoji_list in es_emoji:\n",
    "    emoji_list_embedding = []  # Initialize a list for embeddings of each emoji list\n",
    "    for emoji in emoji_list:\n",
    "        try:\n",
    "            emoji_list_embedding.append(emoji_model[emoji])\n",
    "        except KeyError:\n",
    "            pass\n",
    "    if len(emoji_list_embedding) != 0:\n",
    "        emoji_list_embedding = np.concatenate(emoji_list_embedding, axis=0)\n",
    "\n",
    "    es_emoji_embedding.append(emoji_list_embedding)\n",
    "\n",
    "es_max_size = max(len(arr) for arr in es_emoji_embedding)\n",
    "es_padded_arrays = [np.pad(arr, (0, es_max_size - len(arr)), 'constant') for arr in es_emoji_embedding]\n",
    "es_emoji_array = np.vstack(es_padded_arrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940cccfa",
   "metadata": {},
   "source": [
    "## Concatenate the text embeddings and emoji embeddings (if there are more than one emoji, we concatenate all of them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d0c33e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Concatenate along columns (horizontally)\n",
    "en_embeddings = np.concatenate((en_text_array, en_emoji_array), axis=1)\n",
    "es_embeddings = np.concatenate((es_text_array, es_emoji_array), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6b9108",
   "metadata": {},
   "source": [
    "### Padded_arrays may affect clustering result. What else can we do to normalize data with varying size of embeddings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a679dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4624"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "908e063d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9424"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(es_embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344979b9",
   "metadata": {},
   "source": [
    "## Try clustering concatenated word + emoji embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "67ef0aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6812e2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mbkmeans_clusters(\n",
    "\tX, \n",
    "    k, \n",
    "    mb, \n",
    "    print_silhouette_values, \n",
    "):\n",
    "    \"\"\"Generate clusters and print Silhouette metrics using MBKmeans\n",
    "\n",
    "    Args:\n",
    "        X: Matrix of features.\n",
    "        k: Number of clusters.\n",
    "        mb: Size of mini-batches.\n",
    "        print_silhouette_values: Print silhouette values per cluster.\n",
    "\n",
    "    Returns:\n",
    "        Trained clustering model and labels based on X.\n",
    "    \"\"\"\n",
    "    km = MiniBatchKMeans(n_clusters=k, batch_size=mb).fit(X)\n",
    "    print(f\"For n_clusters = {k}\")\n",
    "    print(f\"Silhouette coefficient: {silhouette_score(X, km.labels_):0.2f}\")\n",
    "    print(f\"Inertia:{km.inertia_}\")\n",
    "\n",
    "    if print_silhouette_values:\n",
    "        sample_silhouette_values = silhouette_samples(X, km.labels_)\n",
    "        print(f\"Silhouette values:\")\n",
    "        silhouette_values = []\n",
    "        for i in range(k):\n",
    "            cluster_silhouette_values = sample_silhouette_values[km.labels_ == i]\n",
    "            silhouette_values.append(\n",
    "                (\n",
    "                    i,\n",
    "                    cluster_silhouette_values.shape[0],\n",
    "                    cluster_silhouette_values.mean(),\n",
    "                    cluster_silhouette_values.min(),\n",
    "                    cluster_silhouette_values.max(),\n",
    "                )\n",
    "            )\n",
    "        silhouette_values = sorted(\n",
    "            silhouette_values, key=lambda tup: tup[2], reverse=True\n",
    "        )\n",
    "        for s in silhouette_values:\n",
    "            print(\n",
    "                f\"    Cluster {s[0]}: Size:{s[1]} | Avg:{s[2]:.2f} | Min:{s[3]:.2f} | Max: {s[4]:.2f}\"\n",
    "            )\n",
    "    return km, km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "24f6195e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 7\n",
      "Silhouette coefficient: 0.07\n",
      "Inertia:363.6427202724333\n",
      "Silhouette values:\n",
      "    Cluster 1: Size:14 | Avg:0.33 | Min:0.20 | Max: 0.42\n",
      "    Cluster 4: Size:61 | Avg:0.30 | Min:0.10 | Max: 0.40\n",
      "    Cluster 5: Size:40 | Avg:0.26 | Min:0.08 | Max: 0.36\n",
      "    Cluster 3: Size:16 | Avg:0.23 | Min:-0.00 | Max: 0.37\n",
      "    Cluster 6: Size:11 | Avg:0.11 | Min:-0.10 | Max: 0.20\n",
      "    Cluster 2: Size:31 | Avg:-0.09 | Min:-0.24 | Max: -0.00\n",
      "    Cluster 0: Size:107 | Avg:-0.15 | Min:-0.26 | Max: -0.02\n"
     ]
    }
   ],
   "source": [
    "clustering, cluster_labels = mbkmeans_clusters(\n",
    "\tX=en_embeddings,\n",
    "    k=7,\n",
    "    mb=500,\n",
    "    print_silhouette_values=True,\n",
    ")\n",
    "df_clusters = pd.DataFrame({\n",
    "    \"text\": en_df['text'],\n",
    "    \"tokens\": [\" \".join(text) for text in en_filtered_comments],\n",
    "    \"cluster\": cluster_labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c8f9b6e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You are the best ladyğŸ˜»ğŸ¤©ğŸ™ğŸ‘â™¥ï¸âš½ğŸ¤ fifa.',\n",
       " 'This nikka sick wit itğŸ”¥ğŸ˜‚',\n",
       " \"@shintia97s love over hate !! I'm sorry that you've got so much anger inside for another human. But I forgive you and send you love and support. ğŸ«‚ğŸ«‚ğŸ¤—ğŸ¤—ğŸ˜˜ğŸ˜˜ğŸ³ï¸\\u200dğŸŒˆğŸ³ï¸\\u200dğŸŒˆ\",\n",
       " '@afdhalulrizki42 ğŸ˜‚ Stupid Football yes ğŸ˜‚',\n",
       " '@the_queen_of_adventure Haya bina ila lmondial ğŸ˜‚ğŸ‡¨ğŸ‡²ğŸ˜',\n",
       " 'Wow nice moves roboğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ challenge accepted ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥',\n",
       " 'African should stop joining FIFA games is such a shame awwğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢',\n",
       " 'I really wanna join them but Iâ€™m a ChineseğŸ˜­ğŸ˜­ğŸ˜­',\n",
       " 'Dudeâ€¦ YOUR team was so rude and had poor sportsmanship. This new generation think they are entitledğŸ˜¡ Any other country would have been honored to be in the world cup to play on the same field with these GREAT players from ğŸ‡­ğŸ‡· \\n\\nI hope this humbles them. Canada player were acting like little brats. Croatia shut them up and put them on time out. Literally OUTğŸ˜‚ğŸ’ªğŸ»ğŸ‡­ğŸ‡·ğŸ‡­ğŸ‡·ğŸ‡­ğŸ‡·ğŸ‡­ğŸ‡·ğŸ‡­ğŸ‡·ğŸ‡­ğŸ‡·ğŸ†ğŸ†ğŸ†ğŸ†âš½ï¸âš½ï¸âš½ï¸ğŸ‡­ğŸ‡·ğŸ‡­ğŸ‡·ğŸ‡­ğŸ‡·ğŸ‡­ğŸ‡·ğŸ‡­ğŸ‡·ğŸ‡­ğŸ‡·ğŸ‡­ğŸ‡·ğŸ‡­ğŸ‡·ğŸ‡­ğŸ‡·',\n",
       " 'Missing Italy in the world cupğŸ˜ğŸ˜ğŸ˜ğŸ‡®ğŸ‡¹ğŸ‡®ğŸ‡¹ğŸ‡®ğŸ‡¹',\n",
       " '@anchan_0722_ ğŸ˜‚ğŸ˜‚ğŸ˜‚',\n",
       " '@izayahlopez1118 omg so sad an upset little inferior American ğŸ˜‚ğŸ˜‚',\n",
       " 'Right back at you ğŸ˜‚ğŸ˜‚',\n",
       " '@fine.kln it took you two days to think how to come back at me lol and do you even call this â€œfineâ€ thing a joke ğŸ˜‚ honey ğŸ¤­ğŸ˜‚',\n",
       " 'They themselves said it portugal suiğŸ˜‚ğŸ˜‚',\n",
       " '@richard.escobarc by ğŸ˜‚ğŸ˜‚ğŸ‡²ğŸ‡¦ğŸ’ª',\n",
       " '@nacho.cagiao imagine riding off the back of a game from 2014 ğŸ˜‚ğŸ˜‚',\n",
       " \"@mr.talls.rainbow.world you look at this lolmanğŸ˜… a man fallin' love to women, make 'happy family, and absolutely for 'future, and this reality from the God.. Don't be stupid human with the lol+stupid+destroy thingkedğŸ˜…ğŸ˜…ğŸ˜…\",\n",
       " '@theonlyzeee__ ahahaha stpid brazil lost ğŸ˜‚ğŸ˜‚',\n",
       " '@farzad_1818 pretty soon that will be \\U0001faf1ğŸ¾\\u200d\\U0001faf2ğŸ½âœˆï¸ğŸ‡³ğŸ‡±ğŸ‡§ğŸ‡·ğŸ‡ªğŸ‡¸ ğŸ˜‚ğŸ˜‚ğŸ˜‚',\n",
       " '@lucascosmepdm what happened my friend ğŸ˜‚ğŸ˜‚ğŸ‡¦ğŸ‡·ğŸ¥±',\n",
       " '@robertmikocevic yes ğŸ˜‰ğŸ˜‰ğŸ”¥',\n",
       " 'We Respect Japan ğŸ™ŒğŸ™ŒğŸ™ŒğŸ”¥ğŸ”¥',\n",
       " 'France â¤ï¸ğŸ˜and Argentina ğŸ˜â¤ï¸â¤ï¸',\n",
       " '@bigstern50 nah morocco will win cause of referees VAR FIFA they need messi to win the world cup means more money expect foul calls, penalties , free kicks for morocco calling it now -(professional sports) put money on it while you can you might be rich ğŸ˜‚ğŸ˜‚',\n",
       " '@guilherme_tornich ğŸ’ªğŸ‡¦ğŸ‡·ğŸ‡¦ğŸ‡·ğŸ‡¦ğŸ‡·ğŸ˜‚ğŸ˜‚',\n",
       " '@yayatjie09 ğŸ˜‚ğŸ˜‚ ay yâ€™all praise this guy too much ğŸ¤¦ğŸ¾\\u200dâ™‚ï¸',\n",
       " '@areez.kxn go watch all the games ğŸ™ŒğŸ˜‚',\n",
       " '@2k7.aa sadly all good things must come to an end but he will be taking this World Cup with him ğŸ˜ğŸ˜‚',\n",
       " '@_fmbe_ What do you say? ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ‡¦ğŸ‡·ğŸ‡¦ğŸ‡·ğŸ‡¦ğŸ‡·ğŸ‡¦ğŸ‡·',\n",
       " '@anthonyengel7111 Are you still sure?ğŸ˜…ğŸ˜…ğŸ˜…ğŸ˜…']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clusters[df_clusters['cluster']==2]['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "258cc250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements assigned to each cluster: [ 37 113  10  20  36  17  47]\n",
      "Number of elements assigned to each cluster: [ 24  82 120  23  22   5   4]\n",
      "Number of elements assigned to each cluster: [  9 145  20   1  35  13  57]\n",
      "Number of elements assigned to each cluster: [160   6  18  86   5   3   2]\n",
      "Number of elements assigned to each cluster: [123  36  83   2  11  12  13]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seed in range(5):\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=7,\n",
    "        max_iter=100,\n",
    "        n_init=1,\n",
    "        random_state=seed,\n",
    "    ).fit(es_embeddings)\n",
    "    cluster_ids, cluster_sizes = np.unique(kmeans.labels_, return_counts=True)\n",
    "    print(f\"Number of elements assigned to each cluster: {cluster_sizes}\")\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
