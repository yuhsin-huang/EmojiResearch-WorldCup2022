{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03977c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import emoji\n",
    "import warnings\n",
    "import nltk\n",
    "from nltk.corpus import stopwords    \n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54a696f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eafc0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data/cleaned_by_language.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdb35f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_df = df[df['language'] == 'en']\n",
    "es_df = df[df['language'] == 'es']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16da499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_comment = en_df['text only'].to_list()\n",
    "es_comment = es_df['text only'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96a0ab7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/yuhsinhuang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yuhsinhuang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "en_stop_words = set(stopwords.words('english'))\n",
    "en_filtered_comments = []\n",
    "\n",
    "for comment in en_comment:\n",
    "    # remove word starts with @\n",
    "    comment = \" \".join(filter(lambda x:x[0]!='@', comment.split()))\n",
    "    \n",
    "    # remove punctuations\n",
    "    comment = comment.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # remove stopwords\n",
    "    words = nltk.word_tokenize(comment)\n",
    "    filtered_comment = [word for word in words if word.lower() not in en_stop_words]\n",
    "    filtered_comment = ' '.join(filtered_comment)\n",
    "    filtered_comment = comment\n",
    "    en_filtered_comments.append(filtered_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0edf613",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_stop_words = set(stopwords.words('spanish'))\n",
    "es_filtered_comments = []\n",
    "\n",
    "for comment in es_comment:\n",
    "    # remove word starts with @\n",
    "    comment = \" \".join(filter(lambda x:x[0]!='@', comment.split()))\n",
    "    \n",
    "    # remove punctuations\n",
    "    comment = comment.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # remove stopwords\n",
    "    words = nltk.word_tokenize(comment)\n",
    "    filtered_comment = [word for word in words if word.lower() not in es_stop_words]\n",
    "    filtered_comment = ' '.join(filtered_comment)\n",
    "    filtered_comment = comment\n",
    "    es_filtered_comments.append(filtered_comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d189bc7e",
   "metadata": {},
   "source": [
    "## Text Embeddings using LASER - this pretrained model supports cross-lingual tasks and embeds in setence-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb8a53ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: laserembeddings in /opt/anaconda3/lib/python3.9/site-packages (1.1.2)\n",
      "Requirement already satisfied: transliterate==1.10.2 in /opt/anaconda3/lib/python3.9/site-packages (from laserembeddings) (1.10.2)\n",
      "Requirement already satisfied: subword-nmt<0.4.0,>=0.3.6 in /opt/anaconda3/lib/python3.9/site-packages (from laserembeddings) (0.3.8)\n",
      "Requirement already satisfied: sacremoses==0.0.35 in /opt/anaconda3/lib/python3.9/site-packages (from laserembeddings) (0.0.35)\n",
      "Requirement already satisfied: torch<2.0.0,>=1.0.1.post2 in /opt/anaconda3/lib/python3.9/site-packages (from laserembeddings) (1.11.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.15.4 in /opt/anaconda3/lib/python3.9/site-packages (from laserembeddings) (1.21.5)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.9/site-packages (from sacremoses==0.0.35->laserembeddings) (8.0.4)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.9/site-packages (from sacremoses==0.0.35->laserembeddings) (4.63.0)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.9/site-packages (from sacremoses==0.0.35->laserembeddings) (1.1.0)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.9/site-packages (from sacremoses==0.0.35->laserembeddings) (1.16.0)\n",
      "Requirement already satisfied: mock in /opt/anaconda3/lib/python3.9/site-packages (from subword-nmt<0.4.0,>=0.3.6->laserembeddings) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/lib/python3.9/site-packages (from torch<2.0.0,>=1.0.1.post2->laserembeddings) (4.7.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install laserembeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "814bf8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this in terminal first: python -m laserembeddings download-models\n",
    "from laserembeddings import Laser\n",
    "\n",
    "laser = Laser()\n",
    "en_text_array = laser.embed_sentences(en_filtered_comments, lang='en')\n",
    "es_text_array = laser.embed_sentences(es_filtered_comments, lang='es')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7b356d",
   "metadata": {},
   "source": [
    "## Emoji Embeddings using Emoji2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c71241e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "en_emoji = en_df['emoji list'].to_list()\n",
    "\n",
    "# Load pretrained emoji embeddings\n",
    "emoji_model = KeyedVectors.load_word2vec_format('emoji2vec.bin', binary=True)\n",
    "\n",
    "# Initialize a list to store emoji embeddings\n",
    "en_emoji_embedding = []\n",
    "\n",
    "for emoji_list in en_emoji:\n",
    "    emoji_list_embedding = []  # Initialize a list for embeddings of each emoji list\n",
    "    for emoji in emoji_list:\n",
    "        try:\n",
    "            emoji_list_embedding.append(emoji_model[emoji])\n",
    "        except KeyError:\n",
    "            pass\n",
    "    if len(emoji_list_embedding) != 0:\n",
    "        emoji_list_embedding = np.concatenate(emoji_list_embedding, axis=0)\n",
    "    en_emoji_embedding.append(emoji_list_embedding)\n",
    "\n",
    "\n",
    "en_max_size = max(len(arr) for arr in en_emoji_embedding)\n",
    "en_padded_arrays = [np.pad(arr, (0, en_max_size - len(arr)), 'constant') for arr in en_emoji_embedding]\n",
    "en_emoji_array = np.vstack(en_padded_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4421f4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_emoji = es_df['emoji list'].to_list()\n",
    "\n",
    "es_emoji_embedding = []\n",
    "\n",
    "for emoji_list in es_emoji:\n",
    "    emoji_list_embedding = []  # Initialize a list for embeddings of each emoji list\n",
    "    for emoji in emoji_list:\n",
    "        try:\n",
    "            emoji_list_embedding.append(emoji_model[emoji])\n",
    "        except KeyError:\n",
    "            pass\n",
    "    if len(emoji_list_embedding) != 0:\n",
    "        emoji_list_embedding = np.concatenate(emoji_list_embedding, axis=0)\n",
    "\n",
    "    es_emoji_embedding.append(emoji_list_embedding)\n",
    "\n",
    "es_max_size = max(len(arr) for arr in es_emoji_embedding)\n",
    "es_padded_arrays = [np.pad(arr, (0, es_max_size - len(arr)), 'constant') for arr in es_emoji_embedding]\n",
    "es_emoji_array = np.vstack(es_padded_arrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940cccfa",
   "metadata": {},
   "source": [
    "## Concatenate the text embeddings and emoji embeddings (if there are more than one emoji, we concatenate all of them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d0c33e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Concatenate along columns (horizontally)\n",
    "en_embeddings = np.concatenate((en_text_array, en_emoji_array), axis=1)\n",
    "es_embeddings = np.concatenate((es_text_array, es_emoji_array), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6b9108",
   "metadata": {},
   "source": [
    "### Padded_arrays may affect clustering result. What else can we do to normalize data with varying size of embeddings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7a679dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4624"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "908e063d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9424"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(es_embeddings[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3912jvsc74a57bd040d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
